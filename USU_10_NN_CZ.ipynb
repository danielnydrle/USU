{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umělé neuronové sítě typu MLP\n",
    "\n",
    "Pro získání bonusového bodu je potřeba dosáhnout s ReLU aktivační funkci úspěšnosti > 90%. Pro tento účel je nutné odladit hodnotu parametru $\\alpha$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'xTest', 'y', 'yTest', 'w1', 'w2']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "npzfile = np.load('data/data_10.npz')\n",
    "npzfile.files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = npzfile['x']\n",
    "xTest = npzfile['xTest']\n",
    "\n",
    "y = npzfile['y']\n",
    "yTest = npzfile['yTest']\n",
    "\n",
    "x.shape, y.shape\n",
    "\n",
    "w1test = npzfile['w1']\n",
    "w2test = npzfile['w2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkce sigmoid\n",
    "\n",
    "$$ sigmoid(u) = \\sigma (u) = \\frac{e^u}{1+e^u} = \\frac{1}{1+e^{-u}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(u):\n",
    "    #################################################################\n",
    "    # ZDE DOPLNIT\n",
    "    \n",
    "    sig = 1 / (1 + np.exp(-u))\n",
    "\n",
    "    return sig\n",
    "\n",
    "    #################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73105858, 0.88079708],\n",
       "       [0.04742587, 0.01798621]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Kontrola:\n",
    "u = np.array([[1,2],[-3,-4]])\n",
    "sigmoid(u)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "array([[0.73105858, 0.88079708],\n",
    "       [0.04742587, 0.01798621]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivace funkce sigmoid:\n",
    "$$ \\sigma' (u) = \\sigma (u) (1 - \\sigma(u)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(u):\n",
    "    \n",
    "    #################################################################\n",
    "    # ZDE DOPLNIT\n",
    "    \n",
    "    grad = sigmoid(u) * (1 - sigmoid(u))\n",
    "    \n",
    "    return grad\n",
    "\n",
    "    #################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19661193, 0.10499359],\n",
       "       [0.04517666, 0.01766271]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Kontrola:\n",
    "sigmoid_grad(u)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "array([[0.19661193, 0.10499359],\n",
    "       [0.04517666, 0.01766271]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "\n",
    "$$ f(u) = max(0, u) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(u):\n",
    "    #################################################################\n",
    "    # ZDE DOPLNIT\n",
    "\n",
    "    relu = np.maximum(0, u)\n",
    "    \n",
    "    return relu\n",
    "    #################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Kontrola:\n",
    "relu(u)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "array([[1, 2],\n",
    "       [0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivace funkce ReLU:\n",
    "$$ f'(x) = \\boldsymbol{1} (x \\ge 0)$$\n",
    "\n",
    "Derivace přímo v bodě nula je dodefinována na hodnotu nula.\n",
    "\n",
    "Gradient se přes tento blok přenáší:\n",
    "1) Nezměněný, pokud je hodnota na vstupu z dopředného průchodu větší než nula.\n",
    "2) Přenesená hodnota je nula, pokud je hodnota na vstupu z dopředného průchodu menší nebo rovna nule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(u):\n",
    "    #################################################################\n",
    "    # ZDE DOPLNIT\n",
    "\n",
    "    grad = u >= 0\n",
    "\n",
    "    return np.array(grad, dtype=bool)\n",
    "    #################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True],\n",
       "       [False, False]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Kontrola:\n",
    "relu_grad(u)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "array([[ True,  True],\n",
    "       [False, False]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "$ \\pi $ nabývá hodnoty 1 pouze pro jednu třídu. Např. máme celkem 3 třídy (0, 1, 2): $\\pi_0 = [0,1,0]$  pro $y_0 = 1$\n",
    "\n",
    "\n",
    "$$\n",
    "    classes = \n",
    "        \\begin{bmatrix}\n",
    "        1 \\\\\n",
    "        0 \\\\\n",
    "        2\\\\\n",
    "        1 \\\\\n",
    "        \\end{bmatrix} \n",
    "    \\implies\n",
    "        \\pi = \n",
    "        \\begin{bmatrix}\n",
    "        0 & 1 & 0 \\\\\n",
    "        1 & 0 & 0 \\\\\n",
    "        0 & 0 & 1 \\\\\n",
    "        0 & 1 & 0 \\\\\n",
    "        \\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(data):\n",
    "    #################################################################\n",
    "    # ZDE DOPLNIT\n",
    "\n",
    "    n_classes = len(np.unique(data))\n",
    "\n",
    "    one_hot = np.zeros((len(data), n_classes))\n",
    "    one_hot[np.arange(len(data)), data.flatten()] = 1\n",
    "    \n",
    "    return one_hot\n",
    "    #################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Kontrola:\n",
    "encoded = one_hot_encoding(y)\n",
    "encoded[[0,900,1800,2700,3500,4200],:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "- Funkce softmax má c vstupů a c výstupů. \n",
    "- Všechny výstupy jsou kladná čísla. \n",
    "- Součet všech výstupů dohromady je roven číslu 1.\n",
    "$$\\widehat{y_c} = softmax(u) = \\frac{e^{u_c}}{\\sum_{d=0}^{c} {e^{u_d}}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(u):\n",
    "    \"\"\"\n",
    "    softmax !radkove!\n",
    "    \"\"\"\n",
    "    #################################################################\n",
    "    # ZDE DOPLNIT\n",
    "\n",
    "    y = np.exp(u) / np.sum(np.exp(u), axis=1)[:, np.newaxis]\n",
    "    \n",
    "    return y \n",
    "    #################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26894142, 0.73105858],\n",
       "       [0.73105858, 0.26894142]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Kontrola:\n",
    "softmax(u)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "array([[0.26894142, 0.73105858],\n",
    "       [0.5       , 0.5       ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    " def theta_grad(grad_on_output, input_data):\n",
    "    #################################################################\n",
    "    # ZDE DOPLNIT\n",
    "    weight_grad = (input_data.T @ grad_on_output).T\n",
    "    bias_grad = np.sum(grad_on_output, axis=0)\n",
    "    #################################################################\n",
    "    return weight_grad, bias_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4)\n",
      "(2, 3)\n",
      "(3, 4)\n",
      "[[43 48 20  9]\n",
      " [32 36 16  6]\n",
      " [75 84 36 15]]\n",
      "(3,)\n",
      "[5 4 9]\n"
     ]
    }
   ],
   "source": [
    "#test vypoctu gradientu pro matici vah a biasy\n",
    "\n",
    "#dva vstupni vektory, kazdy ma 4 hodnoty, cili jde o vrstvu, kde kazdy neuron ma 4 vstupy\n",
    "input_test = np.array([[7,8,4,1],[9,10,4,2]])\n",
    "print(input_test.shape)\n",
    "\n",
    "#dva gradienty na vystupu, kazdy ma 3 hodnoty, cili jde o vrstvu, ktera ma 3 neurony [a kazdy ma 4 vstupy])\n",
    "grad_on_output_test = np.array([[1,2,3],[4,2,6]])\n",
    "print(grad_on_output_test.shape)\n",
    "\n",
    "w_grad_test,u_grad_test = theta_grad(grad_on_output_test,input_test)\n",
    "\n",
    "#gradienu vektoru vah ma tedy rozmery 3*4\n",
    "print(w_grad_test.shape)\n",
    "print(w_grad_test)\n",
    "\n",
    "#gradient biasu ma 3 hodnoty\n",
    "print(u_grad_test.shape)\n",
    "print(u_grad_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(2, 4)\n",
    "(2, 3)\n",
    "(3, 4)\n",
    "[[43 48 20  9]\n",
    " [32 36 16  6]\n",
    " [75 84 36 15]]\n",
    "(3,)\n",
    "[5 4 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sítě typu vícevrstvý perceptron = Multi-Layer Perceptron (MLP)\n",
    "\n",
    "\n",
    "\n",
    "### Předzpracování dat\n",
    "Pro trénování neuronových sítí je vhodné provádět standardizaci dat na nulovou střední hodnotu a jednotkový rozptyl.\n",
    "\n",
    "### Inicializace parametrů (váhových koeficientů)\n",
    "- Váhy neuronů nesmí být nastaveny na stejné hodnoty (např. 0), aby neměly stejnou hodnotu výstupu a stejný gradient\n",
    "=>\n",
    "- Je třeba porušit symetrii:\n",
    "    - Váhy se inicializují jako malá náhodná čísla (polovina kladná, polovina záporná)\n",
    "    - V praxi se pro ReLU používá hodnota $randn(n) * sqrt(2.0/n)$, kde n je počet vstupů neuronu\n",
    "    - Započítání počtu vstupů pak zajišťuje, že neurony s různým počtem vstupů mají výstup se stejným rozptylem hodnot\n",
    "    - Biasy se inicializují na hodnotu 0 nebo 0.01 (symetrie je již porušena inicializací váhových koeficientů)\n",
    "\n",
    "### Dopředný průchod\n",
    "Kroky:\n",
    "1. $u_1 = \\theta_1^T x_1$ (vstupní vrstva)\n",
    "2. $a_1 = ReLU(u_1)$ (aktivační funkce)\n",
    "3. $u_2 = \\theta_2^T a_1$ (skrytá vrstva)\n",
    "4. $\\tilde{y} = softmax(u_2)$ (výstupní vrstva)\n",
    "\n",
    "Na výstupu vznikne podle zvoleného kritéria chyba či odchylka\n",
    "\n",
    "### Zpětný průchod\n",
    "\n",
    "(hodnoty z dopředného průhodu $a_1$ a $u_2$ je vhodné si během dopředného průchodu uložit)\n",
    "\n",
    "1. $du_2 = softmax(x)-\\pi(y)$\n",
    "\n",
    "2. $dW_2 = du_2^T a_1$  a  $db_2 = du_2 $\n",
    "\n",
    "3. $da_1 = W_2^T du_2$\n",
    "\n",
    "4. $du_1 = relu'(da_1) = relu'(W_2^T du_2)$\n",
    "\n",
    "5. $dW_1 = du_1^T x$  a  $db = du_1 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# ZDE DOPLNIT\n",
    "class TwoLayerPerceptron:\n",
    "    def __init__ (self, input_data, classes, \n",
    "                  test_data, test_classes,\n",
    "                  input_layer_size, hidden_layer_size, output_size,\n",
    "                  activation_function, activation_function_derivation, alpha=0.00015, lmbd=0):\n",
    "    \n",
    "        self.input_data = input_data\n",
    "        self.classes = classes\n",
    "        \n",
    "        self.test_data = test_data\n",
    "        self.test_classes = test_classes\n",
    "        \n",
    "        self.w1 = np.random.rand(hidden_layer_size, input_layer_size + 1) * np.sqrt(2. / input_layer_size)\n",
    "        self.w2 = np.random.rand(output_size, hidden_layer_size + 1) * np.sqrt(2. / hidden_layer_size)\n",
    "        \n",
    "        self.activation_function = activation_function\n",
    "        self.activation_function_derivation = activation_function_derivation\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.lmbd = lmbd\n",
    "        \n",
    "    def forward(self, mode='training'):\n",
    "        if(mode=='training'):\n",
    "            x = self.input_data\n",
    "        else:\n",
    "            x = self.test_data\n",
    "\n",
    "        #1. vrstva\n",
    "        x1 = np.hstack((np.ones((x.shape[0],1)),x))\n",
    "        u1 = x1 @ self.w1.T\n",
    "\n",
    "        #aktivacni funkce pomocí funkce self.activation_function\n",
    "        a1 = self.activation_function(u1)\n",
    "\n",
    "        #2. vrstva (skryta vrstva)\n",
    "        x2 = np.hstack((np.ones((a1.shape[0],1)),a1))\n",
    "        u2 = x2 @ self.w2.T\n",
    "\n",
    "        #vystup po softmaxu\n",
    "        scores = softmax(u2)\n",
    "        \n",
    "        #cache\n",
    "        self.scores = scores\n",
    "        self.a1 = a1\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def backward(self):\n",
    "                \n",
    "        \n",
    "        #pomocí self.scores, self.classes a one_hot_encoding \n",
    "        du2 = self.scores - one_hot_encoding(self.classes)\n",
    "        \n",
    "        #pomocí funkce theta_grad\n",
    "        dw2, db2 = theta_grad(du2, self.a1)\n",
    "        \n",
    "        da1 = du2 @ self.w2[:,1:]\n",
    "        \n",
    "        #pomocí funkce self.activation_function_derivation\n",
    "        #POZOR: tato funkce se musí aplikovat na vstupní hodnotu z dopředného průchodu !!\n",
    "        du1 = da1 * self.activation_function_derivation(self.a1)\n",
    "        \n",
    "        #pomocí funkce theta_grad\n",
    "        dw1, db1 = theta_grad(du1, self.input_data)\n",
    "        \n",
    "        #POZOR: dw2 není gradient celé matice w2 ale pouze její části\n",
    "        #gradient celé matice w2 pro update vah vznikne vhodným spojením dw2 a db2\n",
    "\n",
    "        w2 = self.w2 * (1 - self.alpha * self.lmbd) - self.alpha * np.c_[db2,dw2]\n",
    "        \n",
    "        #obdobně dw1 není gradient celé matice w1 !!\n",
    "        w1 = self.w1 * (1 - self.alpha * self.lmbd) - self.alpha * np.c_[db1,dw1]\n",
    "        \n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        \n",
    "        return w1, w2 \n",
    "    \n",
    "    def accuracy(self):\n",
    "        scores = self.forward('test')\n",
    "        predicted_classes = np.argmax(scores,axis=1)\n",
    "        return (predicted_classes==self.test_classes.T).mean() * 100\n",
    "    \n",
    "    def setWeigtsForTest(self,w1,w2):\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "            \n",
    "#################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size = 400\n",
    "hidden_layer_size = 25\n",
    "output_size = len(np.unique(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.04783135e-06 -6.15187918e-06 -3.09972922e-05 -5.11502892e-04\n",
      " -1.01684694e-03 -1.00431299e-03  1.41514706e-04]\n",
      "[-0.68741076 -1.94362559  2.01300712 -3.12207333 -0.23513146  1.38975155\n",
      "  0.91141916 -1.54754314 -0.79837387 -0.65466578  0.73622662 -2.58579641\n",
      "  0.47383578  0.55547437  2.51500361 -2.41635527 -1.63847029  1.20323884\n",
      " -1.20416007 -1.83481622 -1.88023829 -0.33927671  0.23811071 -1.05911533\n",
      "  1.02886739 -0.47560228]\n"
     ]
    }
   ],
   "source": [
    "#Instance pro odladění:\n",
    "\n",
    "testTlp = TwoLayerPerceptron(x, y, xTest, yTest, \n",
    "                         input_layer_size, hidden_layer_size, output_size, \n",
    "                         sigmoid, sigmoid_grad, \n",
    "                         alpha = 0.0005, lmbd=0)\n",
    "testTlp.setWeigtsForTest(w1test,w2test)\n",
    "\n",
    "scores = testTlp.forward()\n",
    "# print(testTlp.a1[0]) \n",
    "# print(scores[3]) \n",
    "w1, w2 = testTlp.backward()\n",
    "print(w1[1,3:10])\n",
    "print(w2[2,:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pro ladění:\n",
    "forward:\n",
    "x1[:,0:4]\n",
    "[[1. 0. 0. 0.]\n",
    " [1. 0. 0. 0.]\n",
    " [1. 0. 0. 0.]\n",
    " ...\n",
    " [1. 0. 0. 0.]\n",
    " [1. 0. 0. 0.]\n",
    " [1. 0. 0. 0.]]\n",
    "u1[:,0:4]\n",
    "[[ 4.49064643 -9.04210233  2.84027269  4.37206593]\n",
    " [ 0.81171744 -1.89711703  1.85454769  2.70242974]\n",
    " [-4.92204922  1.62505363 -1.63987317  3.34948622]\n",
    " ...\n",
    " [-5.11646089 -2.62684399  4.66054153  5.41543074]\n",
    " [-0.7061599  -2.91871491 -1.52511379 -0.22418914]\n",
    " [ 1.76313448 -6.87709282  3.74244317  3.69964681]]\n",
    "a1[0]\n",
    "[9.88910953e-01 1.18307825e-04 9.44813682e-01 9.87532276e-01\n",
    " 9.61459910e-01 1.31154023e-02 9.79568046e-01 7.05262715e-01\n",
    " 5.76467884e-03 9.19662205e-01 9.97263993e-01 9.08968353e-01\n",
    " 3.54340337e-03 1.54639813e-01 3.48157918e-03 9.64035515e-01\n",
    " 6.28337854e-04 7.80228020e-03 1.46816393e-01 9.29857033e-01\n",
    " 9.88740224e-01 4.21889681e-02 9.95978182e-01 1.23596237e-02\n",
    " 3.08083417e-02]\n",
    "x2[:,0:4]\n",
    "[[1.00000000e+00 9.88910953e-01 1.18307825e-04 9.44813682e-01]\n",
    " [1.00000000e+00 6.92475360e-01 1.30435117e-01 8.64660171e-01]\n",
    " [1.00000000e+00 7.23151291e-03 8.35490908e-01 1.62482321e-01]\n",
    " ...\n",
    " [1.00000000e+00 5.96145805e-03 6.74306419e-02 9.90627340e-01]\n",
    " [1.00000000e+00 3.30447917e-01 5.12361344e-02 1.78709722e-01]\n",
    " [1.00000000e+00 8.53601797e-01 1.03007524e-03 9.76852371e-01]]\n",
    "u2[:,0:4]\n",
    "[[ -9.3676932   -6.38107877 -12.6179903    5.60001973]\n",
    " [ -2.87539171  -2.53341895  -6.38845969  -4.79830061]\n",
    " [ -5.1814231   -7.11402377   2.61876464  -9.11359119]\n",
    " ...\n",
    " [ -5.58171484  -3.30391879  -4.94701057 -10.62168707]\n",
    " [ -1.27735949  -0.34901449  -3.49339797  -8.81367006]\n",
    " [ -5.51804473  -4.09046134  -9.65245961  -6.42219279]]\n",
    "scores[3]\n",
    "[1.70049726e-05 1.30427018e-04 6.67761717e-07 2.30092641e-02\n",
    " 5.11252228e-03 3.23657653e-04 2.31894141e-04 5.87981895e-04\n",
    " 9.70496699e-01 8.98808528e-05]\n",
    " \n",
    "backward:\n",
    "du2[3,0:4]\n",
    "[1.70049726e-05 1.30427018e-04 6.67761717e-07 2.30092641e-02]\n",
    "dw2[3,0:4]\n",
    "[-1.03220314 -2.1725677  -0.64194913  0.6108072 ]\n",
    "db2[0:4]\n",
    "[ 8.88586683 -1.67675408 -3.85991324 -0.11891478]\n",
    "da1[1,0:4]\n",
    "[-0.0516748   0.03879986  0.01025437 -0.01587243]\n",
    "du1[1,0:4]\n",
    "[-0.01148586  0.00965883  0.00213838 -0.00321013]\n",
    "dw1[3,0:4]\n",
    "[0.00000000e+00 0.00000000e+00 2.93908380e-08 3.99038593e-05]\n",
    "db1[0:4]\n",
    "[-1.83808322 -1.89853117 -0.26354143  1.66833278]\n",
    "\n",
    "výsledek:\n",
    "w1[1,3:10]\n",
    "[ 1.04783135e-06 -6.15187918e-06 -3.09972922e-05 -5.11502892e-04\n",
    " -1.01684694e-03 -1.00431299e-03  1.41514706e-04]\n",
    "w2[2,:]\n",
    "[-0.68741076 -1.94362559  2.01300712 -3.12207333 -0.23513146  1.38975155\n",
    "  0.91141916 -1.54754314 -0.79837387 -0.65466578  0.73622662 -2.58579641\n",
    "  0.47383578  0.55547437  2.51500361 -2.41635527 -1.63847029  1.20323884\n",
    " -1.20416007 -1.83481622 -1.88023829 -0.33927671  0.23811071 -1.05911533\n",
    "  1.02886739 -0.47560228]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid testovaci mnozina : 78.0\n"
     ]
    }
   ],
   "source": [
    "tlp = TwoLayerPerceptron(x, y, xTest, yTest, \n",
    "                         input_layer_size, hidden_layer_size, output_size, \n",
    "                         sigmoid, sigmoid_grad, \n",
    "                         alpha = 0.0005, lmbd=0.002)\n",
    "\n",
    "nIter = 150\n",
    "#################################################################\n",
    "\n",
    "for i in range(nIter):\n",
    "            \n",
    "    scores = tlp.forward()\n",
    "    w1, w2 = tlp.backward()\n",
    "    \n",
    "pred = tlp.accuracy()\n",
    "#################################################################\n",
    "print(f\"sigmoid testovaci mnozina : {pred}\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid testovaci mnozina : 89.0 alpha : 9e-05\n",
      "sigmoid testovaci mnozina : 88.6 alpha : 9.1e-05\n",
      "sigmoid testovaci mnozina : 87.6 alpha : 9.2e-05\n",
      "sigmoid testovaci mnozina : 88.6 alpha : 9.3e-05\n",
      "sigmoid testovaci mnozina : 88.8 alpha : 9.4e-05\n",
      "sigmoid testovaci mnozina : 88.0 alpha : 9.499999999999999e-05\n",
      "sigmoid testovaci mnozina : 89.0 alpha : 9.599999999999999e-05\n",
      "sigmoid testovaci mnozina : 89.2 alpha : 9.699999999999999e-05\n",
      "sigmoid testovaci mnozina : 89.4 alpha : 9.799999999999998e-05\n",
      "sigmoid testovaci mnozina : 88.0 alpha : 9.899999999999998e-05\n",
      "sigmoid testovaci mnozina : 88.2 alpha : 9.999999999999998e-05\n",
      "sigmoid testovaci mnozina : 88.6 alpha : 0.00010099999999999997\n",
      "sigmoid testovaci mnozina : 87.8 alpha : 0.00010199999999999997\n",
      "sigmoid testovaci mnozina : 70.6 alpha : 0.00010299999999999997\n",
      "sigmoid testovaci mnozina : 89.0 alpha : 0.00010399999999999997\n",
      "sigmoid testovaci mnozina : 76.0 alpha : 0.00010499999999999996\n",
      "sigmoid testovaci mnozina : 88.8 alpha : 0.00010599999999999996\n",
      "sigmoid testovaci mnozina : 52.0 alpha : 0.00010699999999999996\n",
      "sigmoid testovaci mnozina : 88.6 alpha : 0.00010799999999999996\n",
      "sigmoid testovaci mnozina : 90.2 alpha : 0.00010899999999999995\n",
      "sigmoid testovaci mnozina : 50.4 alpha : 0.00010999999999999995\n",
      "sigmoid testovaci mnozina : 66.60000000000001 alpha : 0.00011099999999999995\n",
      "sigmoid testovaci mnozina : 47.599999999999994 alpha : 0.00011199999999999994\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nIter):\n\u001b[1;32m     15\u001b[0m     scores \u001b[38;5;241m=\u001b[39m tlp\u001b[38;5;241m.\u001b[39mforward()\n\u001b[0;32m---> 16\u001b[0m     w1, w2 \u001b[38;5;241m=\u001b[39m \u001b[43mtlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m pred \u001b[38;5;241m=\u001b[39m tlp\u001b[38;5;241m.\u001b[39maccuracy()\n\u001b[1;32m     19\u001b[0m graph\u001b[38;5;241m.\u001b[39mappend((_r, pred))\n",
      "Cell \u001b[0;32mIn[63], line 59\u001b[0m, in \u001b[0;36mTwoLayerPerceptron.backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#pomocí funkce theta_grad\u001b[39;00m\n\u001b[1;32m     57\u001b[0m dw2, db2 \u001b[38;5;241m=\u001b[39m theta_grad(du2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma1)\n\u001b[0;32m---> 59\u001b[0m da1 \u001b[38;5;241m=\u001b[39m \u001b[43mdu2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw2\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#pomocí funkce self.activation_function_derivation\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#POZOR: tato funkce se musí aplikovat na vstupní hodnotu z dopředného průchodu !!\u001b[39;00m\n\u001b[1;32m     63\u001b[0m du1 \u001b[38;5;241m=\u001b[39m da1 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_function_derivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma1)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "r = np.arange(0.00009, 0.001, 0.000001)\n",
    "graph = []\n",
    "\n",
    "for _r in r:\n",
    "    tlp = TwoLayerPerceptron(x, y, xTest, yTest, \n",
    "                         input_layer_size, hidden_layer_size, output_size, \n",
    "                         relu, relu_grad, \n",
    "                         alpha = _r, lmbd=0.002)\n",
    "\n",
    "    nIter = 150\n",
    "    #################################################################\n",
    "\n",
    "    for i in range(nIter):\n",
    "\n",
    "        scores = tlp.forward()\n",
    "        w1, w2 = tlp.backward()\n",
    "    \n",
    "    pred = tlp.accuracy()\n",
    "    graph.append((_r, pred))\n",
    "    print(f\"sigmoid testovaci mnozina : {pred} alpha : {_r}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot([x[0] for x in graph], [x[1] for x in graph])\n",
    "plt.show()\n",
    "# tlp = TwoLayerPerceptron(x, y, xTest, yTest, \n",
    "#                          input_layer_size, hidden_layer_size, output_size,\n",
    "#                          relu, relu_grad, 0.00008, 0)\n",
    "\n",
    "# nIter = 150\n",
    "#################################################################\n",
    "\n",
    "# for i in range(nIter):\n",
    "            \n",
    "#     scores = tlp.forward()\n",
    "#     w1, w2 = tlp.backward()\n",
    "    \n",
    "# pred = tlp.accuracy()\n",
    "#################################################################\n",
    "# print(f\"relu testovaci mnozina : {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
